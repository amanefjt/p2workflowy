# 書籍モード開発の振り返り（Retrospective）

## 1. 開発の背景と目的
当初、論文モード（Paper Mode）での単一ファイル処理において、数万文字を超える「超長文」の際にAIが出力の限界に達し、内容を勝手に要約したり途中で打ち切ったりする問題が発生しました。これを解決し、さらに数百ページに及ぶ「書籍」を安定して処理するために、書籍モードの開発に着手しました。

## 2. 採用したアーキテクチャ: Map-Split-Reuse パターン
単一の巨大なプロンプトで全てを処理するのではなく、以下の4段階に分離する設計を採用しました。

1.  **Phase 1: Full-Text Mapping**
    *   書籍全文から「章のタイトル」と、その章の「冒頭の一節（アンカーテキスト）」を抽出。
    *   これをJSON形式で出力し、後続のプログラム処理のインデックスとする。
2.  **Phase 3: Anchor-Based Splitting**
    *   抽出したアンカーテキストを「物理的な目印」として、プログラム（Python）側でテキストを正確に分割。
    *   LLM側のコンテキスト制限や出力制限に依存せず、確実に章ごとのファイルを作成。
3.  **Phase 3: Reuse Paper Mode (Sequential Processing)**
    *   分割された各章を、既存の「論文モード（3フェーズ：レジュメ→構造化→翻訳）」に投入。
    *   各章を独立したタスクとして扱うことで、精度の劣化を防ぐ。
4.  **Phase 4: Mechanical Merging**
    *   処理済みの各章を、Workflowy形式の階層構造（テンプレート）に従って機械的に結合。

## 3. 直面した課題と解決策
*   **出力トークンの壁と「スタミナ不足」**:
    *   Gemini 1.5/3 Flash は入力には強いが、一度に出力できるのは文字数にして1.5万〜2万文字程度が限界。これを超えると、AIは「残りを要約して終わらせる」というショートカットを選ぶ傾向がある。
    *   対策: 論文モードでも「構造化フェーズ（Phase 2）」を1.5万文字単位でチャンク分割するように変更。
*   **ヒントによる汚染（Hint Poisoning）**:
    *   詳細な日本語レジュメを構造化のヒントとして渡すと、AIが「ヒントにある日本語の見出し」をそのまま英語ドキュメントに挿入したり、ヒントでまとめられた章を本文でも結合してしまう問題が発生。
    *   対策: プロンプトに「日本語を無視」「原文の英語見出しを復元」という強力な制約を導入。

## 4. 論文モードへの影響と結論
書籍モードのための「チャンク処理」や「強力なヒント指示」は、当初は論文モードの汎用性を高めると思われましたが、逆に「AIの自由度」を奪い、細かな整合性を合わせるための「思考コスト（負荷）」を増大させる結果となりました。

単一の論文（数万文字程度）であれば、以前の「一括処理に近いシンプルな制御」の方が、AIのスタミナ配分がうまく機能し、結果として自然な全文出力が得られることが分かりました。

## 5. アーカイブの内容
*   `src/book_processor.py`: アンカーベースの分割ロジック。
*   `archive/book_mode/shared/prompts.json`: 書籍用の目次抽出プロンプト等を含む。
*   `archive/book_mode/src/main.py`: 書籍対応版のメインロジック。
