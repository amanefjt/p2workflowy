# -*- coding: utf-8 -*-
"""
p2workflowy - è‹±èªè«–æ–‡ãƒ»æ›¸ç±å‡¦ç†ãƒ—ãƒ­ã‚°ãƒ©ãƒ 
"""
import sys
import asyncio
import argparse
import json
import re
import shutil
from typing import List, Dict, Any, cast
from pathlib import Path
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

from .skills import PaperProcessorSkills
from .utils import Utils
from .constants import EXCLUDE_SECTION_KEYWORDS
from .book_processor import map_book_toc, split_by_anchors
from .llm_processor import LLMProcessor


def print_progress(message: str, percentage: int | None = None) -> None:
    """é€²æ—ã‚’è¡¨ç¤ºã™ã‚‹"""
    if percentage is not None:
        # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¡¨ç¤ºæ™‚ã¯ã€è¡Œã®æœ€å¾Œã«ã‚«ãƒ¼ã‚½ãƒ«ã‚’ç½®ã„ã¦ä¸Šæ›¸ãå¯èƒ½ã«ã™ã‚‹
        print(f"\r[{percentage:3d}%] {message}", end="", flush=True)
    else:
        # \r ã§è¡Œé ­ã«æˆ»ã‚Šã€å‰å›ã®è¡¨ç¤ºã‚’ä¸Šæ›¸ãã—ã¦ã‹ã‚‰æ”¹è¡Œã™ã‚‹ã€‚
        # å¤ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæ®‹ã‚‰ãªã„ã‚ˆã†ã€ã‚¹ãƒšãƒ¼ã‚¹ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ã€‚
        print(f"\r{message:<80}")


async def run_paper_pipeline(input_file: Path, skills: PaperProcessorSkills, glossary_text: str):
    """è«–æ–‡ãƒ¢ãƒ¼ãƒ‰ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    output_final = input_file.parent / f"{input_file.stem}_output.txt"
    output_structured = input_file.parent / f"{input_file.stem}_structured_eng.md"

    raw_text = Utils.read_text_file(input_file)

    # Phase 1: Semantic Mapping (ãƒ¬ã‚¸ãƒ¥ãƒ¡ç”Ÿæˆ)
    print_progress("Phase 1: åŸæ–‡ã‹ã‚‰æ„å‘³çš„ãªæ§‹é€ ï¼ˆãƒ¬ã‚¸ãƒ¥ãƒ¡ï¼‰ã‚’æŠŠæ¡ä¸­...", 10)
    resume_text = await skills.generate_resume(
        raw_text,
        progress_callback=lambda msg: print_progress(f"Phase 1: {msg}")
    )
    print_progress("Phase 1: ãƒ¬ã‚¸ãƒ¥ãƒ¡ç”Ÿæˆå®Œäº†", 30)

    # Phase 2: Anchored Structuring (æ§‹é€ åŒ–)
    print_progress("Phase 2: ãƒ¬ã‚¸ãƒ¥ãƒ¡ã‚’ã‚¬ã‚¤ãƒ‰ã«ã—ã¦åŸæ–‡ã®æ§‹é€ ã‚’å¾©å…ƒä¸­...", 30)
    structure_hint = Utils.extract_structure_from_resume(resume_text)
    structured_md = await skills.structure_text_with_hint(
        raw_text,
        structure_hint,
        progress_callback=lambda msg: print_progress(f"Phase 2: {msg}")
    )
    Utils.write_text_file(output_structured, structured_md)
    print_progress("Phase 2: æ§‹é€ åŒ–å®Œäº†", 50)

    # è¿½åŠ : ç¿»è¨³å‰ã«ä¸è¦ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç‰©ç†çš„ã«å‰Šé™¤ (References ç­‰)
    structured_md = Utils.remove_unwanted_sections(structured_md, EXCLUDE_SECTION_KEYWORDS)

    # Phase 3: Contextual Translation (ä¸¦åˆ—ç¿»è¨³)
    print_progress("Phase 3: æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸä¸¦åˆ—ç¿»è¨³ã‚’å®Ÿæ–½ä¸­...", 50)
    translated_text = await skills.translate_academic(
        structured_md,
        glossary_text,
        summary_context=resume_text,
        progress_callback=lambda msg: print_progress(f"Phase 3: {msg}")
    )
    print_progress("Phase 3: ç¿»è¨³å®Œäº†", 90)

    # Phase 4: Assembly (çµåˆ)
    print_progress("Phase 4: æˆæœç‰©ã‚’çµ±åˆä¸­...", 90)
    resume_workflowy = Utils.markdown_to_workflowy(resume_text)
    resume_section = "  - ãƒ¬ã‚¸ãƒ¥ãƒ¡ (Resume)\n" + "\n".join(["    " + line for line in resume_workflowy.splitlines()])

    # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
    eng_lines = structured_md.splitlines()
    title = input_file.stem
    if eng_lines and eng_lines[0].strip().startswith('# '):
        title = eng_lines[0].strip().replace('# ', '').strip()

    # ç¿»è¨³çµæœã®å‡¦ç†
    lines = translated_text.splitlines()
    if lines and lines[0].strip().startswith('# '):
        lines = lines[1:]
    body_text_no_title = "\n".join(lines).strip()
    translation_workflowy = Utils.markdown_to_workflowy(body_text_no_title)
    translation_section = "\n".join(["  " + line for line in translation_workflowy.splitlines()])

    final_content = f"- {title}\n{resume_section}\n{translation_section}"
    Utils.write_text_file(output_final, final_content)
    
    print_progress("Phase 4: å‡¦ç†å®Œäº†!", 100)
    print(f"\næˆæœç‰©: {output_final}")


async def process_single_chapter(
    chapter_idx: int,
    total_chapters: int,
    chapter_title: str,
    chapter_text: str,
    skills: PaperProcessorSkills,
    glossary_text: str,
    context_guide: str = "",
) -> tuple[str, str]:
    """
    1ã¤ã®ç« ã‚’è«–æ–‡ãƒ¢ãƒ¼ãƒ‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å‡¦ç†ã™ã‚‹ï¼ˆPhase 3 ã®å€‹åˆ¥å‡¦ç†å˜ä½ï¼‰ã€‚
    å„ç« ã‚’ç‹¬ç«‹ã—ãŸã€Œè«–æ–‡ã€ã¨ã¿ãªã—ã€PaperProcessorSkills ã®3ãƒ•ã‚§ãƒ¼ã‚ºã‚’é©ç”¨ã™ã‚‹ã€‚
    
    Returns:
        (resume_text, translated_text)
    """
    prefix = f"  [ç¬¬{chapter_idx + 1}ç« /{total_chapters}ç«  '{chapter_title}']"
    print_progress(f"{prefix} å‡¦ç†é–‹å§‹...")

    try:
        # Phase 3a: ãƒ¬ã‚¸ãƒ¥ãƒ¡ç”Ÿæˆï¼ˆç« å˜ä½ï¼‰
        print_progress(f"{prefix} ãƒ¬ã‚¸ãƒ¥ãƒ¡ç”Ÿæˆä¸­...")
        resume_text = await skills.generate_resume(
            chapter_text,
            context_guide=context_guide,
            progress_callback=lambda msg: print_progress(f"{prefix} Resume: {msg}")
        )

        # Phase 3b: æ§‹é€ åŒ–ï¼ˆç« å˜ä½ï¼‰
        print_progress(f"{prefix} æ§‹é€ åŒ–ä¸­...")
        structure_hint = Utils.extract_structure_from_resume(resume_text)
        structured_md = await skills.structure_text_with_hint(
            chapter_text,
            structure_hint,
            context_guide=context_guide,
            progress_callback=lambda msg: print_progress(f"{prefix} Structure: {msg}")
        )

        # è¿½åŠ : ç¿»è¨³å‰ã«ä¸è¦ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç‰©ç†çš„ã«å‰Šé™¤ (References ç­‰)
        structured_md = Utils.remove_unwanted_sections(structured_md, EXCLUDE_SECTION_KEYWORDS)

        # Phase 3c: ç¿»è¨³ï¼ˆç« å˜ä½ â€” ãƒãƒ£ãƒ³ã‚¯ã®ä¸¦åˆ—å‡¦ç†ã¯ translate_academic å†…éƒ¨ã§å®Ÿè¡Œï¼‰
        print_progress(f"{prefix} ç¿»è¨³ä¸­...")
        translated_text = await skills.translate_academic(
            structured_md,
            glossary_text,
            summary_context=resume_text,
            context_guide=context_guide,
            progress_callback=lambda msg: print_progress(f"{prefix} Translate: {msg}")
        )

        print_progress(f"{prefix} å®Œäº† âœ“")
        return resume_text, translated_text

    except Exception as e:
        error_msg = f"{prefix} ã‚¨ãƒ©ãƒ¼: {e}"
        print_progress(error_msg)
        err_ret = f"[ç¬¬{chapter_idx + 1}ç«  '{chapter_title}' ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}]"
        return err_ret, err_ret


async def run_book_pipeline(input_file: Path, skills: PaperProcessorSkills, glossary_text: str):
    """
    æ›¸ç±ãƒ¢ãƒ¼ãƒ‰ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: Map-Split-Reuse ãƒ‘ã‚¿ãƒ¼ãƒ³

    Phase 1: Full-Text Mapping (AI ãŒ ToC + Anchor ã‚’ JSON ã§è¿”ã™)
    Phase 2: Anchor-Based Splitting (ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒã§ç‰©ç†åˆ†å‰²)
    Phase 3: Reuse Paper Mode (å„ç« ã«è«–æ–‡ãƒ¢ãƒ¼ãƒ‰3ãƒ•ã‚§ãƒ¼ã‚ºã‚’é †æ¬¡é©ç”¨ã€ç« å†…ãƒãƒ£ãƒ³ã‚¯ã¯ä¸¦åˆ—)
    Phase 4: Mechanical Merging (ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒªãƒ†ãƒ©ãƒ«ã§çµåˆ)
    """
    output_final = input_file.parent / f"{input_file.stem}_output.txt"

    raw_text = Utils.read_text_file(input_file)
    llm = LLMProcessor()

    # === Phase 0: Book Resume Generation (å…¨ä½“ãƒ¬ã‚¸ãƒ¥ãƒ¡) ===
    print_progress("Phase 0: æ›¸ç±å…¨ä½“ã®ãƒ¬ã‚¸ãƒ¥ãƒ¡ã‚’ç”Ÿæˆä¸­...", 0)
    # æœ¬ã®å†’é ­ï¼ˆIntroductionç­‰ï¼‰ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å…¨ä½“ãƒ¬ã‚¸ãƒ¥ãƒ¡ã‚’ç”Ÿæˆ
    # å…¨æ–‡ã¯é•·ã™ãã‚‹ãŸã‚ã€æœ€åˆã®5000æ–‡å­—ã‚’ä½¿ç”¨
    book_intro_text = raw_text[:5000]
    book_resume = await skills.generate_resume(
        book_intro_text,
        context_guide=f"This is the introductory part of the book '{input_file.stem}'. Please generate a summary for the WHOLE book based on this introduction.",
        progress_callback=lambda msg: print_progress(f"Phase 0: {msg}")
    )
    print_progress("Phase 0: å®Œäº†", 5)

    # === Phase 1: Full-Text Mapping ===
    print_progress("Phase 1: æ›¸ç±å…¨æ–‡ã‹ã‚‰ç›®æ¬¡æ§‹é€ ã‚’è§£æä¸­...", 10)
    toc_mappings = await map_book_toc(
        llm, raw_text,
        progress_callback=lambda msg: print_progress(f"Phase 1: {msg}")
    )
    print_progress(f"Phase 1: å®Œäº† - {len(toc_mappings)}ç« ã‚’æ¤œå‡º", 15)
    for i, m in enumerate(toc_mappings):
        print(f"  {i+1}. {m['chapter_title']}")

    # === Phase 2: Anchor-Based Splitting ===
    print_progress("Phase 2: ã‚¢ãƒ³ã‚«ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã§ç« ã‚’åˆ†å‰²ä¸­...", 20)
    chapters = split_by_anchors(
        raw_text, toc_mappings,
        progress_callback=lambda msg: print_progress(f"Phase 2: {msg}")
    )
    print_progress(f"Phase 2: å®Œäº† - {len(chapters)}ç« ã«åˆ†å‰²", 25)

    # === Phase 3: å„ç« ã‚’é †æ¬¡å‡¦ç†ï¼ˆç« å†…ã®ç¿»è¨³ãƒãƒ£ãƒ³ã‚¯ã¯ä¸¦åˆ—ï¼‰ ===
    print_progress(f"Phase 3: {len(chapters)}ç« ã‚’è«–æ–‡ãƒ¢ãƒ¼ãƒ‰ã§é †æ¬¡å‡¦ç†ä¸­...", 30)

    chapter_results = []
    for i, ch in enumerate(chapters):
        # åºè«–ãªã©ã§ä»–ç« ã¸ã®è¨€åŠãŒè¦‹å‡ºã—ã«ãªã‚‹ã®ã‚’é˜²ããŸã‚ã®ã‚¬ã‚¤ãƒ‰
        context_guide = f"This text is Chapter {i+1} '{ch['title']}' of the book. Do not treat references to other chapters as new headings. Make sure to structure ONLY the content of this chapter."
        
        # æˆ»ã‚Šå€¤ã¯ (resume_text, translated_text) ã®ã‚¿ãƒ—ãƒ«
        result_tuple = await process_single_chapter(
            chapter_idx=i,
            total_chapters=len(chapters),
            chapter_title=ch["title"],
            chapter_text=ch["text"],
            skills=skills,
            glossary_text=glossary_text,
            context_guide=context_guide
        )
        chapter_results.append(result_tuple)

    print_progress("Phase 3: å…¨ç« ã®å‡¦ç†å®Œäº†", 90)

    # === Phase 4: Mechanical Merging (æ©Ÿæ¢°çš„çµåˆ) ===
    print_progress("Phase 4: æˆæœç‰©ã‚’çµ±åˆä¸­...", 90)

    # æ›¸ç±ã‚¿ã‚¤ãƒˆãƒ«ã®æŠ½å‡ºï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã™ã‚‹ï¼‰
    book_title = input_file.stem

    # å…¨ä½“ãƒ¬ã‚¸ãƒ¥ãƒ¡ã® Workflowy å¤‰æ›
    book_resume_wf = Utils.markdown_to_workflowy(book_resume)
    book_resume_section = "  - å…¨ä½“ãƒ¬ã‚¸ãƒ¥ãƒ¡\n" + "\n".join(["    " + line for line in book_resume_wf.splitlines()])

    # å„ç« ã®çµåˆ
    all_chapter_sections = []
    for ch, (ch_resume, ch_translated) in zip(chapters, chapter_results):
        chapter_title = ch["title"]
        
        # ç« ãƒãƒ¼ãƒ‰ç”Ÿæˆ (ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°: ãƒ†ã‚¹ãƒˆå¯èƒ½ã«ã™ã‚‹ãŸã‚é–¢æ•°åŒ–)
        section = format_chapter_node(chapter_title, ch_resume, ch_translated)
        all_chapter_sections.append(section)

    final_content = f"- {book_title}\n{book_resume_section}\n" + "\n".join(all_chapter_sections)
    Utils.write_text_file(output_final, final_content)

    print_progress("Phase 4: å‡¦ç†å®Œäº†!", 100)
    print(f"\næˆæœç‰©: {output_final}")
    print(f"å‡¦ç†ã—ãŸç« æ•°: {len(chapters)}")


def format_chapter_node(chapter_title: str, ch_resume: str, ch_translated: str) -> str:
    """
    ç« ã®ã‚¿ã‚¤ãƒˆãƒ«ã€ãƒ¬ã‚¸ãƒ¥ãƒ¡ã€ç¿»è¨³æœ¬æ–‡ã‚’çµåˆã—ã¦Workflowyå½¢å¼ã®ãƒãƒ¼ãƒ‰æ–‡å­—åˆ—ã‚’ä½œæˆã™ã‚‹
    """
    # ç« ãƒ¬ã‚¸ãƒ¥ãƒ¡
    ch_resume_wf = Utils.markdown_to_workflowy(ch_resume)
    ch_resume_node = "    - ç« ãƒ¬ã‚¸ãƒ¥ãƒ¡\n" + "\n".join(["      " + line for line in ch_resume_wf.splitlines()])

    # ç¿»è¨³æœ¬æ–‡ (H1é™¤å»)
    lines = ch_translated.splitlines()
    if lines and lines[0].strip().startswith('# '):
        lines = lines[1:]
    body = "\n".join(lines).strip()
    
    ch_trans_wf = Utils.markdown_to_workflowy(body)
    # ã€Œæœ¬æ–‡ç¿»è¨³ã€ãƒãƒ¼ãƒ‰ã‚’å‰Šé™¤ã—ã€ç« ã‚¿ã‚¤ãƒˆãƒ«ç›´ä¸‹ã«é…ç½®ï¼ˆã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆ4ã‚¹ãƒšãƒ¼ã‚¹ï¼‰
    ch_trans_node = "\n".join(["    " + line for line in ch_trans_wf.splitlines()])

    # ç« ãƒãƒ¼ãƒ‰çµåˆ
    return f"  - {chapter_title}\n{ch_resume_node}\n{ch_trans_node}"



async def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    project_dir = Path(__file__).parent.parent
    glossary_file = project_dir / "glossary.csv"
    
    # argparseã§ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’å‡¦ç†
    parser = argparse.ArgumentParser(
        description="p2workflowy - è‹±èªè«–æ–‡ãƒ»æ›¸ç±å‡¦ç†",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(
        "input_file",
        nargs="?",
        help="å‡¦ç†å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹"
    )
    parser.add_argument(
        "mode",
        nargs="?",
        choices=["paper", "p", "1", "book", "b", "2"],
        default="paper",
        help="å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: paper/p/1=è«–æ–‡, book/b/2=æ›¸ç±"
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: ä¸­é–“ç”Ÿæˆç‰©ã‚’ <input_file>_test/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜"
    )
    
    args = parser.parse_args()
    
    # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ï¼ˆå¼•æ•°ãªã—ï¼‰
    if not args.input_file:
        print("\n" + "=" * 60)
        print("p2workflowy - è‹±èªè«–æ–‡ãƒ»æ›¸ç±å‡¦ç†")
        print("=" * 60)
        input_path_str = input("ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: ").strip()
        print("\nãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„:")
        print("  1. è«–æ–‡ãƒ¢ãƒ¼ãƒ‰ (paper)")
        print("  2. æ›¸ç±ãƒ¢ãƒ¼ãƒ‰ (book)")
        mode_input = input("ãƒ¢ãƒ¼ãƒ‰ [1]: ").strip() or "1"
        mode = "book" if mode_input in ["2", "book", "b"] else "paper"
        test_mode = False
    else:
        input_path_str = args.input_file
        mode = args.mode
        test_mode = args.test
    
    # ãƒ¢ãƒ¼ãƒ‰ã®æ­£è¦åŒ–
    if mode in ["paper", "p", "1"]:
        mode = "paper"
    elif mode in ["book", "b", "2"]:
        mode = "book"
    
    input_file = Path(input_path_str.strip("'\""))
    if not input_file.exists():
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_file}")
        return

    glossary_text = Utils.load_glossary(glossary_file) if glossary_file.exists() else ""

    print(f"\nå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™... (ãƒ¢ãƒ¼ãƒ‰: {'ğŸ“„ è«–æ–‡' if mode == 'paper' else 'ğŸ“– æ›¸ç±'})")
    skills = PaperProcessorSkills()

    if mode == "book":
        await run_book_pipeline(input_file, skills, glossary_text)
    else:
        await run_paper_pipeline(input_file, skills, glossary_text)


if __name__ == "__main__":
    asyncio.run(main())
